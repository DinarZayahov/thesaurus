{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ThesaurusVisualization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMREOLepInQknuNycfL23fs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "ztSQz-lE-wsA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install minisom\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "KnzMnYKf7Sv4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.0.5\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "9C3JeB3m-rQZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS_FILE = 'stopwords.txt'"
      ],
      "metadata": {
        "id": "9vaxYqErDggq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "\n",
        "def download_model():\n",
        "\n",
        "    filename = 'en_core_web_sm_temporary'\n",
        "    if not os.path.exists(filename):\n",
        "        r = requests.get('https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0'\n",
        "                         '/en_core_web_sm-3.0.0.tar.gz', allow_redirects=True)\n",
        "        open(filename, 'wb').write(r.content)\n",
        "        tar = tarfile.open(filename, 'r:gz')\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "\n",
        "\n",
        "def download_stopwords():\n",
        "  filename = STOPWORDS_FILE\n",
        "  if not os.path.exists(filename):\n",
        "        r = requests.get('https://github.com/DinarZayahov/thesaurus/releases/download/0.0.1/extended_stopwords.txt', allow_redirects=True)\n",
        "        open(filename, 'wb').write(r.content)"
      ],
      "metadata": {
        "id": "P65_g3Un2lIL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_model()\n",
        "download_stopwords()"
      ],
      "metadata": {
        "id": "GBeWAEiNCjwZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from gensim.utils import tokenize\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "from minisom import MiniSom\n",
        "\n",
        "from bokeh.models import ColumnDataSource, HoverTool\n",
        "from bokeh.io import show, output_notebook\n",
        "from bokeh.plotting import figure\n",
        "\n",
        "# display bokeh plot in notebook\n",
        "output_notebook()\n",
        "\n",
        "\n",
        "MAX_LENGTH = 1250000\n",
        "LEMMATIZATION_THRESHOLD = 500000\n",
        "\n",
        "\n",
        "class Thesaurus:\n",
        "    def __init__(self):\n",
        "        self.spacy_model = None\n",
        "\n",
        "    @staticmethod\n",
        "    def read_text(file):\n",
        "        lines = []\n",
        "        for line in file:\n",
        "            # line = line.decode('utf-8', 'ignore')\n",
        "            lines.append(line)\n",
        "        return ''.join(lines)\n",
        "\n",
        "    def set_spacy_model(self, model):\n",
        "        self.spacy_model = spacy.load(model)\n",
        "        self.spacy_model.max_length = MAX_LENGTH\n",
        "\n",
        "    def lemmatize(self, text, length):\n",
        "        if length < LEMMATIZATION_THRESHOLD:\n",
        "            doc = self.spacy_model(text)\n",
        "            result = \" \".join([token.lemma_ for token in doc])\n",
        "            return result\n",
        "        else:\n",
        "            for doc in self.spacy_model.pipe([text], batch_size=32, n_process=3, disable=[\"parser\", \"ner\"]):\n",
        "                result = \" \".join([token.lemma_ for token in doc])\n",
        "                return result\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        tokens = list(tokenize(text, to_lower=True))\n",
        "        return tokens\n",
        "\n",
        "    @staticmethod\n",
        "    def get_stopwords(path):\n",
        "        stopwords_file = open(path, 'r')\n",
        "        stopwords = []\n",
        "        for line in stopwords_file:\n",
        "            stopwords.append(line[:-1])\n",
        "        return stopwords\n",
        "\n",
        "    def remove_stopwords(self, tokens: list):\n",
        "        stopwords = self.get_stopwords(STOPWORDS_FILE)\n",
        "        filtered_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in stopwords:\n",
        "                filtered_tokens.append(token)\n",
        "        return filtered_tokens, list(dict.fromkeys(filtered_tokens))\n",
        "\n",
        "    def make_embeddings(self, tokens: list) -> list:\n",
        "        embeddings_filename = 'embeddings.pickle'\n",
        "        if os.path.exists(embeddings_filename):\n",
        "            print('Found cache..')\n",
        "            embeddings_file = open(embeddings_filename, 'rb')\n",
        "            changed = False\n",
        "            dictionary = pickle.load(embeddings_file)\n",
        "            result = []\n",
        "            for token in tokens:\n",
        "                if token in dictionary:\n",
        "                    result.append(dictionary[token])\n",
        "                else:\n",
        "                    e = self.spacy_model(token).vector\n",
        "                    dictionary[token] = e\n",
        "                    changed = True\n",
        "                    result.append(e)\n",
        "            if changed:\n",
        "                print('Rewriting cache..')\n",
        "                embeddings_file.close()\n",
        "                os.remove(embeddings_filename)\n",
        "                new_embeddings_file = open(embeddings_filename, 'wb')\n",
        "                pickle.dump(dictionary, new_embeddings_file)\n",
        "            return result\n",
        "        else:\n",
        "            print('Cache not found..')\n",
        "            dictionary = dict()\n",
        "            for token in tokens:\n",
        "                dictionary[token] = self.spacy_model(token).vector\n",
        "            embeddings_file = open(embeddings_filename, 'wb')\n",
        "            pickle.dump(dictionary, embeddings_file)\n",
        "            return list(dictionary.values())\n",
        "\n",
        "    @staticmethod\n",
        "    def get_grid_size(n):\n",
        "        neurons_num = 5*np.sqrt(n)\n",
        "        return int(np.ceil(np.sqrt(neurons_num)))\n",
        "\n",
        "    def plot_bokeh(self, embeddings_f, embeddings_b, filtered_ftext_set, filtered_btext_set):\n",
        "        HEXAGON_SIZE = 54\n",
        "        DOT_SIZE = 20\n",
        "\n",
        "        GRID_SIZE = self.get_grid_size(len(embeddings_b))\n",
        "        PLOT_SIZE = HEXAGON_SIZE * (GRID_SIZE + 1)\n",
        "\n",
        "        som = MiniSom(GRID_SIZE, GRID_SIZE, np.array(embeddings_b).shape[1], sigma=5, learning_rate=.2,\n",
        "                      activation_distance='euclidean', topology='hexagonal', neighborhood_function='bubble',\n",
        "                      random_seed=10)\n",
        "\n",
        "        som.train(embeddings_b, 1000, verbose=True)\n",
        "\n",
        "        b_label = []\n",
        "\n",
        "        b_weight_x, b_weight_y = [], []\n",
        "        for cnt, i in enumerate(embeddings_b):\n",
        "            w = som.winner(i)\n",
        "            wx, wy = som.convert_map_to_euclidean(xy=w)\n",
        "            wy = wy * np.sqrt(3) / 2\n",
        "            b_weight_x.append(wx)\n",
        "            b_weight_y.append(wy)\n",
        "            b_label.append(filtered_btext_set[cnt])\n",
        "\n",
        "        f_label = []\n",
        "\n",
        "        f_weight_x, f_weight_y = [], []\n",
        "        for cnt, i in enumerate(embeddings_f):\n",
        "            w = som.winner(i)\n",
        "            wx, wy = som.convert_map_to_euclidean(xy=w)\n",
        "            wy = wy * np.sqrt(3) / 2\n",
        "            f_weight_x.append(wx)\n",
        "            f_weight_y.append(wy)\n",
        "            f_label.append(filtered_ftext_set[cnt])\n",
        "\n",
        "        # initialise figure/plot\n",
        "        fig = figure(plot_height=PLOT_SIZE, plot_width=PLOT_SIZE,\n",
        "                     match_aspect=True,\n",
        "                     tools=\"pan\")\n",
        "\n",
        "        fig.axis.visible = False\n",
        "        fig.xgrid.grid_line_color = None\n",
        "        fig.ygrid.grid_line_color = None\n",
        "\n",
        "        # create data stream for plotting\n",
        "        b_source_pages = ColumnDataSource(\n",
        "            data=dict(\n",
        "                wx=b_weight_x,\n",
        "                wy=b_weight_y,\n",
        "                species=b_label\n",
        "            )\n",
        "        )\n",
        "\n",
        "        f_source_pages = ColumnDataSource(\n",
        "            data=dict(\n",
        "                wx=f_weight_x,\n",
        "                wy=f_weight_y,\n",
        "                species=f_label\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fig.hex(x='wy', y='wx', source=b_source_pages,\n",
        "                fill_alpha=1.0, line_alpha=1.0,\n",
        "                size=HEXAGON_SIZE)\n",
        "\n",
        "        fig.scatter(x='wy', y='wx', source=f_source_pages,\n",
        "                    fill_color='orange',\n",
        "                    size=DOT_SIZE)\n",
        "\n",
        "        TOOLTIPS = \"\"\"\n",
        "            <div style =\"border-style: solid;border-width: 15px;background-color:black;\">         \n",
        "                <div>\n",
        "                    <span style=\"font-size: 12px; color: white;font-family:century gothic;\"> @species</span>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        # add hover-over tooltip\n",
        "        fig.add_tools(HoverTool(\n",
        "            tooltips=[\n",
        "                (\"label\", '@species')],\n",
        "            # tooltips=TOOLTIPS,\n",
        "            mode=\"mouse\",\n",
        "            point_policy=\"follow_mouse\"\n",
        "        ))\n",
        "\n",
        "        return fig  "
      ],
      "metadata": {
        "id": "fKDDkcjl2xDn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vr4gJUrL1qdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "ee3256c5-3249-4c47-d3fe-b615031929fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found cache..\n",
            "Found cache..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f0a13595cdb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0membeddings_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tokens_b_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_bokeh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_tokens_f_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_tokens_b_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-852f4c7d2c67>\u001b[0m in \u001b[0;36mplot_bokeh\u001b[0;34m(self, embeddings_f, embeddings_b, filtered_ftext_set, filtered_btext_set)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mDOT_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mGRID_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_grid_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mPLOT_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHEXAGON_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGRID_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __get_grid_size() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "FOREGROUND_FILE = '2108.06252v1.txt'\n",
        "BACKGROUND_FILE = '2111.06414v1.txt'\n",
        "\n",
        "foreground = open(FOREGROUND_FILE, 'r')\n",
        "background = open(BACKGROUND_FILE, 'r')\n",
        "\n",
        "MODEL = 'en_core_web_sm-3.0.0/en_core_web_sm/en_core_web_sm-3.0.0'\n",
        "\n",
        "if (foreground is not None) and (background is not None):\n",
        "\n",
        "    obj = Thesaurus()\n",
        "\n",
        "    foreground = obj.read_text(foreground)\n",
        "    background = obj.read_text(background)\n",
        "\n",
        "    obj.set_spacy_model(MODEL)\n",
        "\n",
        "    lemmatized_f = obj.lemmatize(foreground, len(foreground))\n",
        "\n",
        "    lemmatized_b = obj.lemmatize(background, len(background))\n",
        "\n",
        "    tokenized_f = obj.tokenize(lemmatized_f)\n",
        "\n",
        "    tokenized_b = obj.tokenize(lemmatized_b)\n",
        "\n",
        "    filtered_tokens_f, filtered_tokens_f_set = obj.remove_stopwords(tokenized_f)\n",
        "    filtered_tokens_b, filtered_tokens_b_set = obj.remove_stopwords(tokenized_b)\n",
        "\n",
        "    embeddings_f = obj.make_embeddings(filtered_tokens_f_set)\n",
        "    embeddings_b = obj.make_embeddings(filtered_tokens_b_set)\n",
        "\n",
        "    fig = obj.plot_bokeh(embeddings_f, embeddings_b, filtered_tokens_f_set, filtered_tokens_b_set)\n",
        "\n",
        "    show(fig)"
      ]
    }
  ]
}